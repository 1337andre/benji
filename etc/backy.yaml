configurationVersion: '0.1'

# Where should the backy logfile be placed?
# Backy will by default log INFO, WARNING and ERROR to this log.
# If you also need DEBUG information, please start backy with '-v'.
logFile: /var/log/backy.log

# Default block size. 4MB are recommended.
# DO NOT CHANGE WHEN BACKUPS EXIST!
blockSize: 4194304

# Hash function to use. Use a large one to avoid collisions.
# DO NOT CHANGE WHEN BACKUPS EXIST!
hashFunction: sha512

# for some operations, full backy or single versions need to be locked for
# simulatanous access. In the lock_dir we will create backy_*.lock files.
lockDirectory: /run

# To be able to find other backys running, we need a system-wide unique name
# for all backy processes.
# DO NOT CHANGE WHILE backy2 PROCESSES ARE RUNNING!
processName: backy2

# Allow rm of backup versions after n days (set to 0 to disable, i.e. to be
# able to delete any version)
disallowRemoveWhenYounger: 6

metaBackend:
  # Of which type is the Metadata Backend Engine?
  # Available types: sql
  type: sql

  sql:
    # Which SQL Server?
    # Available servers:
    #   sqlite:////path/to/sqlitefile
    #   postgresql:///database
    #   postgresql://user:password@host:port/database
    engine: sqlite:////var/lib/backy2/backy.sqlite

dataBackend:
  # Which data backend to use?
  # Available types:
  #   file, s3, s3_boto3, b2
  type: file

  file:
    # Store data to this path. A structure of 2 folders depth will be created
    # in this path (e.g. '0a/33'). Blocks of DEFAULTS.block_size will be stored
    # there. This is your backup storage!
    path: /var/lib/backy2/data

  #s3:
  #  awsAccessKeyId: ********
  #  awsSecretAccessKey: ********
  #  host: www.example.com
  #  port: 80
  #  isSecure: false
  #  bucketName: backy2

  #s3_boto3:
  #  awsAccessKeyId: ********
  #  awsSecretAccessKey: ********
  #  host: www.example.com
  #  port: 80
  #  isSecure: false
  #  bucketName: backy2
  #  multiDelete: true

  #b2:
  #   accountId: ***********
  #   applicationKey: *************
  #   bucketName: backy2
  #   accountInfoFile: .... (optional)

  # How many writes to perform in parallel. This is useful if your backup space
  # can perform parallel writes faster than serial ones.
  simultaneousWrites: 5

  # How many reads to perform in parallel. This is useful if your backup space
  # can perform parallel reads faster than serial ones.
  simultaneousReads: 5

  # Bandwidth throttling (set to 0 to disable, i.e. use full bandwidth)
  # bytes per second
  #bandwidthRead: 78643200
  #bandwidthWrite: 78643200

  #encryption:
  #  - name: aws_s3_cse
  #    materials:
  #      masterKey: !!binary |
  #        e/i1X4NsuT9k+FIVe2kd3vtHVkzZsbeYv35XQJeV8nA=
  #    active: true

  #compression:
  #  - name: zstd
  #    materials:
  #      level: 1
  #    active: true

nbd:
  cacheDirectory: /tmp

io:
  file:
    # Configure the file IO (file://<path>)
    # This is for a file or a blockdevice (e.g. /dev/sda)

    # How many parallel reads are permitted? (also affects the queue length)
    simultaneousReads: 5

  rbd:
    # Configure the rbd IO (rbd://<pool>/<imagename>[@<snapshotname>])
    # This accepts rbd images in the form rbd://pool/image@snapshot or rbd://pool/image
    cephConfigFile: /etc/ceph/ceph.conf

    # How many parallel reads are permitted? (also affects the queue length)
    simultaneousReads: 10

    # When restoring images, new images are created (if you don't --force). For these
    # newly created images, use these features:
    newImageFeatures:
      - RBD_FEATURE_LAYERING
      - RBD_FEATURE_EXCLUSIVE_LOCK
      #- RBD_FEATURE_STRIPINGV2
      #-RBD_FEATURE_OBJECT_MAP
      #-RBD_FEATURE_FAST_DIFF
      #-RBD_FEATURE_DEEP_FLATTEN

